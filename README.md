# Beto Benchmarks

## Summary

## XNLI

### Reported:

#### 73.15 [LASER](https://arxiv.org/abs/1812.10464)
#### 77.8 [Original Multilingual BERT repository](https://github.com/google-research/bert/blob/master/multilingual.md)
#### 78.5 [Multilingual BERT on "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"](https://arxiv.org/pdf/1904.09077.pdf)
#### 80.8 [XLM (MLM+TLM)](https://arxiv.org/pdf/1901.07291.pdf)

### Ours

Best: 80.15

Detailed: experiments_XNLI.txt

## POS

### Reported

#### 96.71 [Multilingual BERT on "How Multilingual is Multilingual BERT?"](https://arxiv.org/pdf/1906.01502.pdf)
#### 97.1 [Multilingual BERT on "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"](https://arxiv.org/pdf/1904.09077.pdf)
#### 98.91 [UDPipe on "75 Languages, 1 Model: Parsing Universal Dependencies Universally"](https://arxiv.org/pdf/1904.02099.pdf)

### Ours

Best: 98.44

Detailed: experiments_POS.txt

## NER CoNLL2002

### Reported

#### 87.18 [Multilingual BERT on "How Multilingual is Multilingual BERT?"](https://arxiv.org/pdf/1906.01502.pdf)
#### 87.38 [Multilingual BERT on "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"](https://arxiv.org/pdf/1904.09077.pdf)

### Ours

Best: 81.7

Detailed: experiments_NER.txt

## NER WikiAnn

### Reported

#### 92.5 [Multilingual BERT on "Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation"](https://arxiv.org/pdf/1906.01569.pdf)

### Ours

## MLDoc

### Reported

#### 88.75 [LASER](https://arxiv.org/abs/1812.10464)
#### 95.7 [Multilingual BERT on "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"](https://arxiv.org/pdf/1904.09077.pdf)

### Ours

## Dependency Parsing

### Reported

#### 92.3/86.5 [Multilingual BERT on "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"](https://arxiv.org/pdf/1904.09077.pdf)

### Ours

## PAWS-X

### Reported

#### 89 and 90.7 [Multilingual BERT on "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"](https://arxiv.org/abs/1908.11828)

### Ours
